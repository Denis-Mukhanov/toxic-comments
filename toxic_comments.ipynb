{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BERT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[![image-1.png](https://i.postimg.cc/PJd3pdbG/image-1.png)](https://postimg.cc/HcP4FDP3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"padding:20px 20px 20px; \n",
    "            color:#004346;\n",
    "            font-size:40px;\n",
    "            display:fill;\n",
    "            text-align:center;\n",
    "            border-radius:20px;\n",
    "            border: 5px double;\n",
    "            border-color:#201E20;\n",
    "            background-color: #E8F1F2;\n",
    "            overflow:hidden;\n",
    "            font-weight:400\"> \n",
    "<p style=\"font-weight: bold; text-align: center;\"> Идентификация токсичных комментариев</p>\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"padding:0px 40px 30px; \n",
    "            color:#004346;\n",
    "            font-size:110%;\n",
    "            display:fill;\n",
    "            border-radius:20px;\n",
    "            border: 5px double;\n",
    "            border-color:#201E20;\n",
    "            background-color: #E8F1F2;\n",
    "            overflow:hidden;\n",
    "            font-weight:450;\"> \n",
    "    \n",
    "__Заказчик:__ Интернет-магазин «Викишоп».\n",
    "    \n",
    "__Постановка задачи:__ Необходимо разработать инструмент, который будет искать токсичные комментарии и отправлять их на модерацию.\n",
    "    \n",
    "__Оценка результата:__ Сравнение качества работы моделей. Значение метрики качества F1 должно быть не меньше 0.75.\n",
    "    \n",
    "__Описание данных:__ Размеченные комментарии. \n",
    "    \n",
    "- `text` - текст комментария\n",
    "- `toxic` — целевой признак\n",
    "    \n",
    "</div>    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"padding:0px 20px 10px; \n",
    "            color:#004346;\n",
    "            font-size:15px;\n",
    "            display:fill;\n",
    "            text-align:center;\n",
    "            border-radius:20px;\n",
    "            border: 5px double;\n",
    "            border-color:#201E20;\n",
    "            background-color: #E8F1F2;\n",
    "            overflow:hidden;\n",
    "            font-weight:400\"> \n",
    "\n",
    "# Используемые библиотеки\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# константы\n",
    "RANDOM_SEED = 42"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"padding:0px 20px 10px; \n",
    "            color:#004346;\n",
    "            font-size:15px;\n",
    "            display:fill;\n",
    "            text-align:center;\n",
    "            border-radius:20px;\n",
    "            border: 5px double;\n",
    "            border-color:#201E20;\n",
    "            background-color: #E8F1F2;\n",
    "            overflow:hidden;\n",
    "            font-weight:400\"> \n",
    "\n",
    "# Загрузка данных\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>toxic</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Explanation\\nWhy the edits made under my usern...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>D'aww! He matches this background colour I'm s...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  toxic\n",
       "0  Explanation\\nWhy the edits made under my usern...      0\n",
       "1  D'aww! He matches this background colour I'm s...      0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# загрузка данных:\n",
    "try:\n",
    "    df = pd.read_csv('data/toxic_comments.csv', index_col='Unnamed: 0')\n",
    "    display(df.head(2))\n",
    "except:\n",
    "    display('Данные не доступны')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"padding:0px 20px 10px; \n",
    "            color:#004346;\n",
    "            font-size:13px;\n",
    "            display:fill;\n",
    "            text-align:center;\n",
    "            border-radius:20px;\n",
    "            border: 5px double;\n",
    "            border-color:#201E20;\n",
    "            background-color: #E8F1F2;\n",
    "            overflow:hidden;\n",
    "            font-weight:400\"> \n",
    "\n",
    "# Предобработка и исследовательский анализ данных\n",
    "\n",
    "## Общая информация\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 159292 entries, 0 to 159450\n",
      "Data columns (total 2 columns):\n",
      " #   Column  Non-Null Count   Dtype \n",
      "---  ------  --------------   ----- \n",
      " 0   text    159292 non-null  object\n",
      " 1   toxic   159292 non-null  int64 \n",
      "dtypes: int64(1), object(1)\n",
      "memory usage: 3.6+ MB\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "None"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(df.info())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Проверим пропуски в данных:__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Общее количество пропусков в данных: 0\n"
     ]
    }
   ],
   "source": [
    "print(f'Общее количество пропусков в данных: {sum(col_pas for col_pas in df.isna().sum())}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Проверим дубликатов в данных:__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Общее количество дубликатов в данных: 0\n"
     ]
    }
   ],
   "source": [
    "print(f'Общее количество дубликатов в данных: {df.index.duplicated().sum()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"padding: 30px 25px; border: 2px #6495ed solid\">\n",
    "\n",
    "- Данные загружены корректно.\n",
    "- Пропуски в данных отсутствуют.\n",
    "- Дублитакы в данных отсутствуют.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Баланс целевого признака__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# баланс целевого признака\n",
    "data = [df['toxic'].value_counts()[0], df['toxic'].value_counts()[1]]\n",
    "labels = ['Положительный текст', 'Отрицательный текст']\n",
    "\n",
    "colors = sns.color_palette('pastel')[ 0:5 ]\n",
    "\n",
    "plt.pie(data, labels = labels, colors = colors, autopct='%.0f%%')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Присутствует дисдаланс классов в целевоц переменной, это надо учесть при обучении моделей."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"padding:0px 20px 10px; \n",
    "            color:#004346;\n",
    "            font-size:15px;\n",
    "            display:fill;\n",
    "            text-align:center;\n",
    "            border-radius:20px;\n",
    "            border: 5px double;\n",
    "            border-color:#201E20;\n",
    "            background-color: #E8F1F2;\n",
    "            overflow:hidden;\n",
    "            font-weight:400\"> \n",
    "\n",
    "## Обработка данных\n",
    "    \n",
    "</div>\n",
    "\n",
    "Для предобработки данных используем предобученную модель BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import transformers\n",
    "from tqdm import notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "404c291ca81b418f9152ae0856828810",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)solve/main/vocab.txt:   0%|          | 0.00/213k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b2e35ef02d174e98b296d7b6540beb7c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)okenizer_config.json:   0%|          | 0.00/29.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5ee4747a54f04b7da84303978efed352",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)lve/main/config.json:   0%|          | 0.00/570 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# инициализация токенизатора\n",
    "tokenizer = transformers.BertTokenizer.from_pretrained('bert-base-cased')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Проверим длинну текстов__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Максимальная длинна текста составляет: 5000 символов\n"
     ]
    }
   ],
   "source": [
    "print(f'Максимальная длинна текста составляет: {max(map(len, df[\"text\"]))} символов')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Максимальная длинна текста составляет: 5 символов\n"
     ]
    }
   ],
   "source": [
    "print(f'Максимальная длинна текста составляет: {min(map(len, df[\"text\"]))} символов')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Стандартная версия модели BERT ('bert-base-cased') имеет ограничение на максимальную длину последовательности, и это ограничение составляет 512 токенов. Это связано с ограниченной памятью графических процессоров (GPU) и вычислительной сложностью обработки более длинных последовательностей при обучении модели.\n",
    "- Мы можем разделить длинные тексты на более короткие части, учитывая минимальную длинну изначальных текстов, тем самым увеличим датафрейм и стандартизируем данные для правильной работы молели BERT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Обработка длинных текстов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "# максимальная длина последовательности\n",
    "max_seq_length = 500\n",
    "# минимальная длина последовательности (5)\n",
    "min_seq_length = min(map(len, df[\"text\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Список стандартизированных строк готов\n"
     ]
    }
   ],
   "source": [
    "# список для новых строк\n",
    "new_rows = []\n",
    "# функция для разделения\n",
    "def split(x):\n",
    "    # разделение на части\n",
    "    chunks = [x['text'][i:i + max_seq_length] for i in range(0, len(x['text']), max_seq_length)]\n",
    "    # cоздание новых строк для каждой части\n",
    "    for chunk in chunks:\n",
    "        if len(chunk) >= min_seq_length:\n",
    "            new_row = {'text': chunk, 'toxic': x['toxic']}\n",
    "            new_rows.append(new_row)\n",
    "            \n",
    "df.apply(split, axis=1)\n",
    "print('Список стандартизированных строк готов')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 224778 entries, 0 to 224777\n",
      "Data columns (total 2 columns):\n",
      " #   Column  Non-Null Count   Dtype \n",
      "---  ------  --------------   ----- \n",
      " 0   text    224778 non-null  object\n",
      " 1   toxic   224778 non-null  int64 \n",
      "dtypes: int64(1), object(1)\n",
      "memory usage: 3.4+ MB\n",
      "None\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>toxic</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Explanation\\nWhy the edits made under my usern...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>D'aww! He matches this background colour I'm s...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  toxic\n",
       "0  Explanation\\nWhy the edits made under my usern...      0\n",
       "1  D'aww! He matches this background colour I'm s...      0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Датасет увеличился на 41%\n"
     ]
    }
   ],
   "source": [
    "# создание обновленного датасета и вывод основной информации\n",
    "df_split = pd.DataFrame(new_rows)\n",
    "print(df_split.info())\n",
    "display(df_split.head(2))\n",
    "print(f'Датасет увеличился на {(len(df_split)-len(df))/len(df):.0%}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Токенизация"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "# токенизация данных\n",
    "tokenized = df_split['text'].apply(lambda x: tokenizer.encode(x, add_special_tokens=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Паддинг данных"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Максимальная длина среди токенов составляет: 502\n"
     ]
    }
   ],
   "source": [
    "# максимальная длина среди токенов\n",
    "max_len = max(map(len, tokenized))\n",
    "print(f' Максимальная длина среди токенов составляет: {max_len}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "# паддинг данных\n",
    "padded = np.array([i + [0]*(max_len - len(i)) for i in tokenized])\n",
    "\n",
    "# создание маски внимания\n",
    "attention_mask = np.where(padded != 0, 1, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "502"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "min(map(len, padded))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Эмбеддинги текста"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-cased were not used when initializing BertModel: ['cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "# загрузка конфигурации\n",
    "config = transformers.BertConfig.from_pretrained('bert-base-cased')\n",
    "\n",
    "# загрузка модели\n",
    "model = transformers.BertModel.from_pretrained('bert-base-cased', config=config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e9a4d1830ee94798ab97d0027a97dc58",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/899 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[84], line 16\u001b[0m\n\u001b[1;32m     13\u001b[0m attention_mask_batch \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mLongTensor(attention_mask[batch_size\u001b[38;5;241m*\u001b[39mi:batch_size\u001b[38;5;241m*\u001b[39m(i\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m)])\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m---> 16\u001b[0m     batch_embeddings \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask_batch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     18\u001b[0m embeddings\u001b[38;5;241m.\u001b[39mappend(batch_embeddings[\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mnumpy())\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/transformers/models/bert/modeling_bert.py:1020\u001b[0m, in \u001b[0;36mBertModel.forward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1011\u001b[0m head_mask \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_head_mask(head_mask, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mnum_hidden_layers)\n\u001b[1;32m   1013\u001b[0m embedding_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membeddings(\n\u001b[1;32m   1014\u001b[0m     input_ids\u001b[38;5;241m=\u001b[39minput_ids,\n\u001b[1;32m   1015\u001b[0m     position_ids\u001b[38;5;241m=\u001b[39mposition_ids,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1018\u001b[0m     past_key_values_length\u001b[38;5;241m=\u001b[39mpast_key_values_length,\n\u001b[1;32m   1019\u001b[0m )\n\u001b[0;32m-> 1020\u001b[0m encoder_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoder\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1021\u001b[0m \u001b[43m    \u001b[49m\u001b[43membedding_output\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1022\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextended_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1023\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1024\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1025\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_extended_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1026\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1027\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1028\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1029\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1030\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1031\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1032\u001b[0m sequence_output \u001b[38;5;241m=\u001b[39m encoder_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m   1033\u001b[0m pooled_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpooler(sequence_output) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpooler \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/transformers/models/bert/modeling_bert.py:610\u001b[0m, in \u001b[0;36mBertEncoder.forward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    601\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39mcheckpoint\u001b[38;5;241m.\u001b[39mcheckpoint(\n\u001b[1;32m    602\u001b[0m         create_custom_forward(layer_module),\n\u001b[1;32m    603\u001b[0m         hidden_states,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    607\u001b[0m         encoder_attention_mask,\n\u001b[1;32m    608\u001b[0m     )\n\u001b[1;32m    609\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 610\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[43mlayer_module\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    611\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    612\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    613\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlayer_head_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    614\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    615\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    616\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    617\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    618\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    620\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m layer_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    621\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m use_cache:\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/transformers/models/bert/modeling_bert.py:495\u001b[0m, in \u001b[0;36mBertLayer.forward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[1;32m    483\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\n\u001b[1;32m    484\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    485\u001b[0m     hidden_states: torch\u001b[38;5;241m.\u001b[39mTensor,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    492\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[torch\u001b[38;5;241m.\u001b[39mTensor]:\n\u001b[1;32m    493\u001b[0m     \u001b[38;5;66;03m# decoder uni-directional self-attention cached key/values tuple is at positions 1,2\u001b[39;00m\n\u001b[1;32m    494\u001b[0m     self_attn_past_key_value \u001b[38;5;241m=\u001b[39m past_key_value[:\u001b[38;5;241m2\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m past_key_value \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 495\u001b[0m     self_attention_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mattention\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    496\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    497\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    498\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    499\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    500\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mself_attn_past_key_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    501\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    502\u001b[0m     attention_output \u001b[38;5;241m=\u001b[39m self_attention_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    504\u001b[0m     \u001b[38;5;66;03m# if decoder, the last output is tuple of self-attn cache\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/transformers/models/bert/modeling_bert.py:434\u001b[0m, in \u001b[0;36mBertAttention.forward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[1;32m    415\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\n\u001b[1;32m    416\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    417\u001b[0m     hidden_states: torch\u001b[38;5;241m.\u001b[39mTensor,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    423\u001b[0m     output_attentions: Optional[\u001b[38;5;28mbool\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    424\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[torch\u001b[38;5;241m.\u001b[39mTensor]:\n\u001b[1;32m    425\u001b[0m     self_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mself(\n\u001b[1;32m    426\u001b[0m         hidden_states,\n\u001b[1;32m    427\u001b[0m         attention_mask,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    432\u001b[0m         output_attentions,\n\u001b[1;32m    433\u001b[0m     )\n\u001b[0;32m--> 434\u001b[0m     attention_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moutput\u001b[49m\u001b[43m(\u001b[49m\u001b[43mself_outputs\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    435\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m (attention_output,) \u001b[38;5;241m+\u001b[39m self_outputs[\u001b[38;5;241m1\u001b[39m:]  \u001b[38;5;66;03m# add attentions if we output them\u001b[39;00m\n\u001b[1;32m    436\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m outputs\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/transformers/models/bert/modeling_bert.py:386\u001b[0m, in \u001b[0;36mBertSelfOutput.forward\u001b[0;34m(self, hidden_states, input_tensor)\u001b[0m\n\u001b[1;32m    384\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdense(hidden_states)\n\u001b[1;32m    385\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout(hidden_states)\n\u001b[0;32m--> 386\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mLayerNorm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43minput_tensor\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    387\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m hidden_states\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/torch/nn/modules/module.py:1494\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1491\u001b[0m             tracing_state\u001b[38;5;241m.\u001b[39mpop_scope()\n\u001b[1;32m   1492\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m result\n\u001b[0;32m-> 1494\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_call_impl\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m   1495\u001b[0m     forward_call \u001b[38;5;241m=\u001b[39m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_slow_forward \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_C\u001b[38;5;241m.\u001b[39m_get_tracing_state() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mforward)\n\u001b[1;32m   1496\u001b[0m     \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m     \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "batch_size = 250\n",
    "embeddings = []\n",
    "\n",
    "# Переносим модель на Metal Performance Shaders (MPS)\n",
    "device = torch.device(\"mps\")\n",
    "model.to(device)\n",
    "\n",
    "for i in notebook.tqdm(range(padded.shape[0] // batch_size)):\n",
    "        batch = torch.LongTensor(padded[batch_size*i:batch_size*(i+1)]).to(device)\n",
    "        attention_mask_batch = torch.LongTensor(attention_mask[batch_size*i:batch_size*(i+1)]).to(device)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            batch_embeddings = model(batch, attention_mask=attention_mask_batch)\n",
    "        \n",
    "        embeddings.append(batch_embeddings[1].cpu().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "print (torch.backends.mps.is_available()) #macOS выше 12.3+ \n",
    "print (torch.backends.mps.is_built()) #MPS активирован"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Чек-лист проверки"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- [x]  Jupyter Notebook открыт\n",
    "- [ ]  Весь код выполняется без ошибок\n",
    "- [ ]  Ячейки с кодом расположены в порядке исполнения\n",
    "- [ ]  Данные загружены и подготовлены\n",
    "- [ ]  Модели обучены\n",
    "- [ ]  Значение метрики *F1* не меньше 0.75\n",
    "- [ ]  Выводы написаны"
   ]
  }
 ],
 "metadata": {
  "ExecuteTimeLog": [
   {
    "duration": 112,
    "start_time": "2023-08-23T14:26:38.893Z"
   }
  ],
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": false,
   "skip_h1_title": false,
   "title_cell": "Содержание",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "165px"
   },
   "toc_section_display": false,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
